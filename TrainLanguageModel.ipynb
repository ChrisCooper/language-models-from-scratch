{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84533cfc-0e45-45bf-94fd-0f6bc3ff1499",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T20:58:17.890177Z",
     "start_time": "2024-02-22T20:58:17.875087Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config for ENV_NAME=dev\n",
      "Reloaded config\n",
      "Loading config for ENV_NAME=dev\n",
      "Reloaded config\n"
     ]
    }
   ],
   "source": [
    "import config, generalizable\n",
    "config.reload_config()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T20:58:19.729925Z",
     "start_time": "2024-02-22T20:58:19.028677Z"
    }
   },
   "id": "97185ed53d65a103"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(1337)\n",
    "device = generalizable.get_best_torch_device()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T20:58:20.366521Z",
     "start_time": "2024-02-22T20:58:20.335187Z"
    }
   },
   "id": "cf23103dac78314e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "667624693725cd2a"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading *.txt from: /Users/chris/datasets/books\n",
      "Loaded 63 files, total length: 48810735\n",
      "Text length: 48810735, total tokens: 13165927, vocab size: 9922\n",
      " WUTHERING HEI\n",
      "torch.Size([13165927]) torch.int64 mps:0\n"
     ]
    }
   ],
   "source": [
    "from custom_tokenizer import FrequencyGreedyTokenizer\n",
    "\n",
    "# raw_text = generalizable.load_text_file('~/datasets/complete_shakespeare.txt')\n",
    "raw_text = generalizable.load_text_directory('~/datasets/books')\n",
    "tokenizer = FrequencyGreedyTokenizer()\n",
    "tokenizer.load(\"vocab.json\")\n",
    "tds = generalizable.encode_text(raw_text, tokenizer=tokenizer)\n",
    "del raw_text\n",
    "full_data = torch.tensor(tds.all_tokens, dtype=torch.long, device=device)\n",
    "print(tds.decode_token_list_to_string(full_data[:10].tolist()))\n",
    "print(full_data.shape, full_data.dtype, full_data.device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T20:58:29.237997Z",
     "start_time": "2024-02-22T20:58:21.985322Z"
    }
   },
   "id": "f3e6785d14fad15f"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "TRAIN_FRACTION = 0.9\n",
    "\n",
    "n = int(TRAIN_FRACTION * len(full_data))\n",
    "train_data = full_data[:n]\n",
    "val_data = full_data[n:]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T20:58:37.565547Z",
     "start_time": "2024-02-22T20:58:37.539544Z"
    }
   },
   "id": "688608b349a1dbb6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "404d096296b41b42"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        # Logits are the raw (non-normalized) predictions that a classification model generates,\n",
    "        # which are then passed to the normalization function, like softmax.\n",
    "        # The cross-entropy loss expects raw logits, not the output of a softmax, so that it can apply its own softmax.\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "        # logits is B*T*C, becuase for each batch, we make `Timestep` predictions, and for each prediction we have C classes\n",
    "        # One character/token predictions for every timestep across the batch sequences\n",
    "        # print(f\"logits.shape: {logits.shape}\")\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Batch, Timestep, Channels\n",
    "            B, T, C = logits.shape\n",
    "            \n",
    "            # cross_entropy expects channels to be the second dimension, so we reshape\n",
    "            # This view squishes the first two dimensions into one, so that the shape becomes (B*T, C)\n",
    "            logits = logits.view(B*T, C)\n",
    "            # print(f\"logits.shape after view: {logits.shape}\")\n",
    "            \n",
    "            # print(f\"targets.shape: {targets.shape}\")\n",
    "            targets = targets.view(B*T)\n",
    "            # print(f\"targets.shape after view: {targets.shape}\")\n",
    "            \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # extract only the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        \n",
    "        return idx"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T20:58:53.292824Z",
     "start_time": "2024-02-22T20:58:53.277920Z"
    }
   },
   "id": "2423274c6a568ca0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create a WandB run"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a14ddeeacc73ad9c"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mchrisc\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.3"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/Users/chris/projects/from-scratch-transformers/wandb/run-20240222_155856-e0chqfle</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/chrisc/language-models/runs/e0chqfle' target=\"_blank\">thriving-paper-21</a></strong> to <a href='https://wandb.ai/chrisc/language-models' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/chrisc/language-models' target=\"_blank\">https://wandb.ai/chrisc/language-models</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/chrisc/language-models/runs/e0chqfle' target=\"_blank\">https://wandb.ai/chrisc/language-models/runs/e0chqfle</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "import humanize\n",
    "\n",
    "LR = 1e-3\n",
    "BLOCK_SIZE = 8\n",
    "BATCH_SIZE = 128\n",
    "NUM_LOSS_ESTIMATE_BATCHES = 400\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"language-models\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning_rate\": LR,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"block_size\": BLOCK_SIZE,\n",
    "        \"model\": \"bigram\",\n",
    "        \"tokenization\": \"greedy_frequency\",\n",
    "    }\n",
    ")\n",
    "\n",
    "m = BigramLanguageModel(tds.vocab_size).to(device)\n",
    "# Learning rate should usually be 1e-4, but for small networks like this, 1e-3 works\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=LR)\n",
    "epochs_trained = 0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T20:58:57.873875Z",
     "start_time": "2024-02-22T20:58:54.916307Z"
    }
   },
   "id": "ff719053fd249952"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2,400, train_loss: 0.045582786202430725, val_loss: 0.045298460870981216\n",
      "  He wes, t He  went in ch e hererinoug staon’ go t hishowevceivewhen own te ads n, aexprieldinid\n",
      "w enoforts de r\n",
      "aow thmone\n",
      "step 4,800, train_loss: 0.04175248742103577, val_loss: 0.04218919202685356\n",
      " [ul d’our Sance. but the bo thesan s had\n",
      "th hKύbertto s bettf\n",
      "e onodech\n",
      "crowisYespirs oned, a ben sudMi\n",
      "step 7,200, train_loss: 0.03910098597407341, val_loss: 0.039747752249240875\n",
      " er o eimber rifEnd cm wall affe. Tbles! ifttansupproddaysaffςs no ύe\n",
      "cat’sl yoks ouressioiness busme o\n",
      "step 9,600, train_loss: 0.036468103528022766, val_loss: 0.037566471844911575\n",
      "  walway,  hop\n",
      "\n",
      "—ut i rect, thing.d. Thpond mi readt by eignt inrrieittle\n",
      "\n",
      "—sonal tatferenld,e an. T, soldih, an to pbly \n",
      "step 12,000, train_loss: 0.034075602889060974, val_loss: 0.03594163432717323\n",
      "  thre olmonth anck passn mes\n",
      "thmar\n",
      "Drely“Itith ad evhad tng at\n",
      "Tperhss.necede,ar, dera h, and I shformount re ofs. A\n",
      "step 14,400, train_loss: 0.032664958387613297, val_loss: 0.03408227115869522\n",
      " usto Arabian. traaiseo conritt di two id, a way b, to\n",
      "his spoeth ark I, al room\n",
      "Closeom hr yer bene wcondm b\n",
      "step 16,800, train_loss: 0.03107423335313797, val_loss: 0.03274166211485863\n",
      " Z tripaw, e dowou sin Conneall; and don’ a ht;the idond nearefaces shver  thanstraighteom\n",
      "an b ho partfil im\n",
      "step 19,200, train_loss: 0.029991645365953445, val_loss: 0.032786790281534195\n",
      " Zaratnt committe a grolorector of the two rie in sve bety.g his\n",
      "took one room er hesame hey sfollow they are, bbut Ior hitas\n",
      "step 21,600, train_loss: 0.029813427478075027, val_loss: 0.03142186999320984\n",
      "  pley doeneraaryizemsa lintra’clothes I woails are we discbe th\n",
      "since and\n",
      "chive! What y ary tot p Mrs.\n",
      "whether \n",
      "step 24,000, train_loss: 0.027726992964744568, val_loss: 0.031030654907226562\n",
      " [seclunded  two famfit, that\n",
      "you, would have giveΘridge under which such a time. Rigram the deerepl spoHAP\n",
      "step 26,400, train_loss: 0.02803879976272583, val_loss: 0.03141176328063011\n",
      " Zayxactlyryt, was sit askRADruning in\n",
      "hearted, thad a milthey apprlevamo, yoy\n",
      "t Thissam Marharp aourse\n",
      "step 28,800, train_loss: 0.027979502454400063, val_loss: 0.02992105297744274\n",
      " [th\n",
      "lenti libst p estorck,, hme. Petercatidemandepoke—nawing the cry o12.]\n",
      "\n",
      "Feath. On\n",
      "step 31,200, train_loss: 0.027253037318587303, val_loss: 0.029737796634435654\n",
      " [Frault was mad,  he r I complished dos ordinglyif the power saidim of\n",
      "Cogilty too. I b\n",
      "“I ick  on to the\n",
      "step 33,600, train_loss: 0.027246816083788872, val_loss: 0.03018876723945141\n",
      " 300 Sefind his\n",
      "seat, immedian capinesting in a hundoes one. inds lodgate, and a speaa coly,e mads ofhtsdown \n",
      "step 36,000, train_loss: 0.02730364352464676, val_loss: 0.03047015517950058\n",
      " 69e.’ drink proposals and he didte of my advice in\n",
      "order to be soms heaoic\n",
      "\n",
      "“W’Adort frego conbeautif\n",
      "step 38,400, train_loss: 0.02604820765554905, val_loss: 0.029312850907444954\n",
      " 8 OFIllustomed to\n",
      "seeing to any neareoreovernment o” said Laussia vor yWhen he\n",
      "had o set  douway \n",
      "step 40,800, train_loss: 0.026103490963578224, val_loss: 0.029694832861423492\n",
      " [Illustomy it,” to yo oper has to\n",
      "have woke again community, to be troop, and beg in mΜyou. And while this last\n",
      "step 43,200, train_loss: 0.025703929364681244, val_loss: 0.028902878984808922\n",
      " —haste,” said he town worse of somethings witr than some mappywn arself, she could weaptain it is from me as it the Read recei\n",
      "step 45,600, train_loss: 0.02608274482190609, val_loss: 0.02900753915309906\n",
      " *\n",
      "\n",
      "And altar enough, yes, sir; now they have this arms;\n",
      "whether\n",
      "vailed upon occorlnvr brother al\n",
      "force the trtion \n",
      "step 48,000, train_loss: 0.025168180465698242, val_loss: 0.02966955676674843\n",
      " ¡A Lachesher i\n",
      "her s, histocracked at the soe here saint aetuit.\n",
      "\n",
      "“‘The calm?\n",
      "\n",
      "When  mustmed tflowers, and b\n",
      "step 50,400, train_loss: 0.025912588462233543, val_loss: 0.029486365616321564\n",
      " XXI?”\n",
      "s; au hav seegiving her\n",
      "than twoe nar prhlak if t imagmy perswaken: ¿qu’dhose same resign\n",
      "step 52,800, train_loss: 0.025965826585888863, val_loss: 0.02983148582279682\n",
      " [ISwitness\n",
      "they havtatioce ho see her,sma fieldest whom I\n",
      "offender arranged they went just what a cloured po\n",
      "step 55,200, train_loss: 0.025052275508642197, val_loss: 0.029315238818526268\n",
      " éclare\n",
      "decliving\n",
      "anything?” she did sos, “my\n",
      "ben, so gent being such terrible that were often I’ve n, “he wa\n",
      "step 57,600, train_loss: 0.026428010314702988, val_loss: 0.028643129393458366\n",
      " égotten ange pon tr asimes. He carefully devrocket some little, om\n",
      "d pae—, y no parents, who did not buman placed be\n",
      "step 60,000, train_loss: 0.025860600173473358, val_loss: 0.030277013778686523\n",
      " ze and ridges, \"Si to leaves only to sing anded for ma back\n",
      "to writ, even Miss From barely happy going to the deep\n",
      "step 62,400, train_loss: 0.025710251182317734, val_loss: 0.028928902000188828\n",
      " --I have tortues, Homerced ualliting his horse not wholeginning of us wakens of your\n",
      "nearent\n",
      "from the shore.\n",
      "Le\n",
      "step 64,800, train_loss: 0.02584974467754364, val_loss: 0.028846176341176033\n",
      " zadation, the lended (ESplenty-yages.”\n",
      "\n",
      "“I should have ren and I sahelmetall.)\n",
      "\n",
      "Do you wish\n",
      "step 67,200, train_loss: 0.02612900733947754, val_loss: 0.0290207602083683\n",
      " x of cis in the most fascinea  him, he only as they werpore y owner of sometimes to the wooday.\"\n",
      "\n",
      "\"I just few moment \n",
      "step 69,600, train_loss: 0.026045942679047585, val_loss: 0.028680752962827682\n",
      " [Of coursomebody doesoint, I am just as\n",
      "at lasta in her have beasseand forgian women herban--At that when he ha\n",
      "step 72,000, train_loss: 0.026171060279011726, val_loss: 0.029275638982653618\n",
      " \n",
      "\n",
      "Certain, and as a wife admirable time!\n",
      "\n",
      "The gratuous as though£os,er,” s dreadén el motion\n",
      "pursugge\n",
      "step 74,400, train_loss: 0.02607167698442936, val_loss: 0.029043275862932205\n",
      " —, none solicity worked that sagaze with ourced\n",
      "hersheath the\n",
      "parebrother fgenehemselves are fond of eachhing n n\n",
      "step 76,800, train_loss: 0.026095857843756676, val_loss: 0.029433980584144592\n",
      " [IXIN Amp, and feltcounsely her.\n",
      "d terrified in my clet the propeselveΠey al int. T\n",
      "look forced \n",
      "step 79,200, train_loss: 0.025389518588781357, val_loss: 0.029429549351334572\n",
      " =æhe feet thing the legaud enough to en el agreeable time he was Paul than EMAgain, old say still he \n",
      "step 81,600, train_loss: 0.025993261486291885, val_loss: 0.028749769553542137\n",
      " * A V\n",
      "\n",
      "Ine, keep the dant of some construck. He is sooking him to abattertained me with them so vailing you’\n",
      "step 84,000, train_loss: 0.025959931313991547, val_loss: 0.02888510189950466\n",
      " Zarathusband,\" I harther\n",
      "had movements.\n",
      "\n",
      "“The progrew surypsed, loom gian tted ihapter he made en s\n",
      "step 86,400, train_loss: 0.02585543878376484, val_loss: 0.028708767145872116\n",
      " écherial.)_ a little walused to doer being and for a Sancho Ansel of it all the first, betwitter false didn’t \n",
      "step 88,800, train_loss: 0.026347875595092773, val_loss: 0.028820302337408066\n",
      " 3. The girl meom hiallerquil di se—25. de with the midlenthe Che’s called for her disquiere P\n",
      "step 91,200, train_loss: 0.02576868049800396, val_loss: 0.028586914762854576\n",
      " ¡, Mr XIMulia’s knees. life is but encies which the ely, para el ciety. She was thus nes, I did n\n",
      "step 93,600, train_loss: 0.025317896157503128, val_loss: 0.029399080201983452\n",
      " 2. _moodily agrolleing spirits were abomin\n",
      "althought ohard indeed, that tly so, ¿Quay, trating so...fro\n",
      "step 96,000, train_loss: 0.026308931410312653, val_loss: 0.029861781746149063\n",
      " Á. Passistingoes  oaken it consortionaelf ta hastó, y no need never rehim. With this much that he mare is suffrsel\n",
      "step 98,400, train_loss: 0.02627999521791935, val_loss: 0.02959800325334072\n",
      " —”\n",
      "\n",
      "“Which there is great exhalmiento del gustomed to\n",
      "bellestia! Or was this play Kings like Directly th\n",
      "\n",
      "\\Final text sample:\n",
      "==============\n",
      "\n",
      "\n",
      " [184453 when itself flujestóniggs. She is a condspirted with his blood of Castle forget\n"
     ]
    }
   ],
   "source": [
    "def get_batch(dataset):\n",
    "    return generalizable.get_batch(dataset, BATCH_SIZE, BLOCK_SIZE, device)\n",
    "\n",
    "def log_wandb_stats(batches_trained):\n",
    "    train_loss = generalizable.estimate_loss(NUM_LOSS_ESTIMATE_BATCHES, lambda : get_batch(train_data), m)\n",
    "    val_loss = generalizable.estimate_loss(NUM_LOSS_ESTIMATE_BATCHES, lambda : get_batch(val_data), m)\n",
    "    wandb.log({\"train_loss\": train_loss, \"val_loss\": val_loss}, step=batches_trained)\n",
    "    return train_loss, val_loss\n",
    "\n",
    "def print_text_sample(num_tokens=30):\n",
    "    idx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "    tokens = m.generate(idx = idx, max_new_tokens=num_tokens)[0].tolist()\n",
    "    print(tds.decode_token_list_to_string(tokens))\n",
    "\n",
    "wandb_interval = 400\n",
    "sample_print_multiplier = 6\n",
    "\n",
    "log_wandb_stats(epochs_trained * BATCH_SIZE)\n",
    "\n",
    "for _, steps in enumerate(range(100000)):\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch(train_data)\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    epochs_trained += 1\n",
    "    \n",
    "    if epochs_trained % wandb_interval == 0:\n",
    "        train_loss, val_loss = log_wandb_stats(epochs_trained * BATCH_SIZE)\n",
    "        \n",
    "        if epochs_trained % (wandb_interval * sample_print_multiplier) == 0:\n",
    "            print(f\"\\n\\nStep {humanize.intcomma(epochs_trained)}, train_loss: {train_loss}, val_loss: {val_loss}\")\n",
    "            print(\"------\")\n",
    "            print_text_sample()\n",
    "\n",
    "print(\"\\n\\Final text sample:\\n==============\\n\\n\")\n",
    "print_text_sample(100)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T23:15:55.354195Z",
     "start_time": "2024-02-22T20:58:58.676123Z"
    }
   },
   "id": "113c4308847c6a0f"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ze:\n",
      "\n",
      "— Yo, se, we don’t me\n",
      "andp tog blastrighrough its flacy, broad, and close and SENO! Siby, Mine! ‘ORuth of the truth.\n",
      "\n",
      "Julies,\n",
      "Tashoreover, she might behavi each symen, who did she put my idea—“No, bendingw height face, id, I am now living betweeelicious course you eat many such tried to twentosura combing the ECzard.\n",
      "\n",
      "Pip\n"
     ]
    }
   ],
   "source": [
    "# idx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "# tokens = m.generate(idx = idx, max_new_tokens=500)[0].tolist()\n",
    "# # print(tds.decode_token_list_to_string(tokens))\n",
    "# print('\\n'.join([tds.int_to_str[tok] for tok in tokens]))\n",
    "# print_text_sample()\n",
    "def print_text_sample(num_tokens=30):\n",
    "    idx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "    tokens = m.generate(idx = idx, max_new_tokens=num_tokens)[0].tolist()\n",
    "    print(tds.decode_token_list_to_string(tokens))\n",
    "\n",
    "print_text_sample(100)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T23:18:26.752748Z",
     "start_time": "2024-02-22T23:18:26.024616Z"
    }
   },
   "id": "e7f39b1ced5713bc"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e8b147c6b84a431ebb388274405c24a2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>█▇▆▅▄▄▃▃▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▇▆▅▄▄▃▃▂▂▂▂▂▁▁▂▁▁▂▁▁▂▁▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>0.02567</td></tr><tr><td>val_loss</td><td>0.0297</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">thriving-paper-21</strong> at: <a href='https://wandb.ai/chrisc/language-models/runs/e0chqfle' target=\"_blank\">https://wandb.ai/chrisc/language-models/runs/e0chqfle</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20240222_155856-e0chqfle/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Finish the WandB run\n",
    "wandb.finish()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T23:18:13.459976Z",
     "start_time": "2024-02-22T23:17:44.574767Z"
    }
   },
   "id": "47e964c6ec05c1ce"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with_chars = \"\"\"\n",
    "Gofuidrme :\n",
    "Toun hu: l:\n",
    "I w herseso thell de de; omole l to.\n",
    "Aneverkined leoumy e thuftxy, wlveres juront lure omy hesph yo harer core' veneayoro ne trsw'll isurendime ilerve fin?\n",
    "\n",
    "BRI meso ot ce pr w o can w'losto h prin hif mem! Soums masan IZBELARomit Mu out ikentharn hen rvaiwit, curencer y ve il\n",
    "UKI I to welilearas be mpure thiks! hatte'd aized'?\n",
    "\n",
    "tar m anju tathe afedind inghevere Yowhare fofot foling a ge wr'farmfien ar aw bellathy feldin bawd henoslendothecendeang--\n",
    "\n",
    "d! ond buthet, miz! \n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "988acd86cf040949"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
